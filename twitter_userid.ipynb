{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python API's is common for websites.\n",
    "Many Internet Companies such as Facebook,Google,twitter provides API(Application programming Interface) that u can use to built your own application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of python API's : https://www.pythonforbeginners.com/api/list-of-python-apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are popular Python libraries for the Twitter API::   \n",
    "        1.Tweepy  \n",
    "        2.Twython   \n",
    "        3.python Twitter Tools  \n",
    "        4.python Twitter  \n",
    "        5.Twitter API   \n",
    "        6.Twitter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to get getting Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit the Application Management page at https://apps.twitter.com/, and sign in with your Twitter account\n",
    "#Click on the \"Create New App\" button, fill in the details and agree the Terms of Service\n",
    "#Navigate to \"Keys and Access Tokens\" section and take a note of your Consumer Key and Secret\n",
    "#In the same section click on \"Create my access token\" button\n",
    "#Take note of your Access Token and Access Token Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app details:https://developer.twitter.com/en/apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv \n",
    "# Here getting all needed credentials(consumer_key,consumer_secert,Access_token,Access_secert)\n",
    "auth = tweepy.auth.OAuthHandler('yTz5GHTTBp6SXJcQlHDt1NIFs', 'XFr04YVnyHJQZifHoPOw6kRRMncFp1atcvYYmIho2wDvXFafbi')\n",
    "auth.set_access_token('1173554885695922177-WOzS8FQ9Wjh2m3ksbPfba0ZJdMfi9O', 'CHgsh2v5TOKNdcRcBwO4ri9Ei6A2W0ruXbzf3BhYQ65Kb')\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Open/create a file to append data to\n",
    "csvFile21 = open('result21.csv', 'a')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile21)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q = \"AadvikAjith\",\n",
    "                           #tweet_mode='extended',\n",
    "                           count=1000,\n",
    "                           lang = \"en\").items():\n",
    "     # Write a row to the CSV file. I use encode UTF-8\n",
    "        csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "        print (tweet.created_at, tweet.text)\n",
    "csvFile21.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Downloads\\\\result21.csv\",names=[\"Date&Time\",\"User_id&msg\"])\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['User_id&msg'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Twitter Handles..\n",
    "import re\n",
    "str1=df['User_id&msg'][0:]\n",
    "print(str1.head(5))                           # Here '@user' called as twitterhandles\n",
    "for i in str1:\n",
    "    User_id=re.findall('@[^:]+',i)\n",
    "    print(User_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all id's\n",
    "user_id = []\n",
    "for i in str1:\n",
    "    User_id=re.findall('@[^:]+',i)\n",
    "    user_id.append(User_id)\n",
    "print(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "user_id1 = pd.DataFrame(user_id)\n",
    "user_id1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving Column names\n",
    "user_id1.rename(columns={0:'userid'},inplace=True)\n",
    "user_id1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all unique id's\n",
    "user_id1['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting count of all unique id's\n",
    "user_id1['userid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=user_id1.groupby('userid')\n",
    "b=a.first()\n",
    "c=a.get_group('@ThalaFC_')\n",
    "d=user_id1.groupby(['userid','msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2=df['User_id&msg'][1:]\n",
    "print(str2.head(3))\n",
    "lst= []\n",
    "for i in str2:\n",
    "    User_id=re.findall(':[^.]+',i)\n",
    "    if User_id:\n",
    "        lst.append(User_id[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "newlst = []\n",
    "for j in lst:\n",
    "        a=j.replace('#',\" \")\n",
    "        a1=a.replace('\\\\n',\" \")\n",
    "        c=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', a1)\n",
    "        if c:\n",
    "            newlst.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=pd.DataFrame(newlst)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.rename(columns={0:'msg'},inplace=True)\n",
    "msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing Concatenation\n",
    "=pd.concat([user_id1,msg],axis=1, join='inner')\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.drop(1,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping function:: Here doing splitting depending on some criteria\n",
    "grouping = a.groupby(['userid','msg'])\n",
    "grouping.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp1=a.groupby('userid')\n",
    "aa1=gp1.first()            #Print the first value in each group\n",
    "aa1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=gp1.get_group('@AKFC_Namakkal')\n",
    "print(a1)\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=gp1.get_group('@AjithNetwork')\n",
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total ids = 827, unique id's=37,      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueid='@AjithNetwork', '@tn_ajith', '@Dheena_shankar', '@RVaralakshmi','@templecityAFC', '@rsofficial18', '@AKFC_Namakkal','@NellaiAFCOnline', '@Ajithkishore10', '@arvndoffcl','@AjithFCMadurai', '@AthulBalendran', '@AnandKa13698179','@vinothmural annan\\\\n\\\\n| #Valimai https', '@murthyyuva','@UmamaheshwarT', '@AjithAathi2', '@BillaPr37327633','@vedalam_kumar31', '@ThalaKMP', '@ARUNTHA01688252', '@AjithFC_VNB', '@dirhvinoth7', '@AjithFc_Nellai','@AjithFC_VELLORE', '@abhishekvarthan', '@attagasambala','@VeeraChennai_of', '@AjithFCDindigul', '@AjithFC_AMBUR','@ThalaFC_', '@ramramram77','@ajithfc @ThalaAjith_Page\\\\xe2\\\\x80\\\\xa6 https', '@igtamil','@AjithFC_INDIA', '@ThalaAjith_FC', '@Rajavel83944647'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in uniqueid:\n",
    "    print('')\n",
    "    print(i)\n",
    "    #print(type(i))\n",
    "    try:\n",
    "        a10=gp1.get_group(i)\n",
    "        a10.shape\n",
    "        print(a10)\n",
    "        print(a10.shape)\n",
    "    except:\n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/create a file to append data to\n",
    "csvFileuserid = open('useidmsg.csv', 'a')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFileuserid)\n",
    "for i in uniqueid:\n",
    "    #print(i)\n",
    "    #print(type(i))\n",
    "    try:\n",
    "        a10=gp1.get_group(i)\n",
    "        #print(a10)\n",
    "        print(a10.shape)\n",
    "    except:\n",
    "        print(0)\n",
    "\n",
    "\n",
    "     # Write a row to the CSV file. I use encode UTF-8\n",
    "    csvWriter.writerow([a10.encode('utf-8')])\n",
    "    print (tweet.created_at, tweet.text)\n",
    "csvFileuserid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in uniqueid:\n",
    "    print('')\n",
    "    print(i)\n",
    "    #print(type(i))\n",
    "    try:\n",
    "        a10=gp1.get_group(i)\n",
    "        a10.shape\n",
    "        print(a10)\n",
    "        print(a10.shape)\n",
    "    except:\n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q = \"AadvikAjith\",\n",
    "                           #tweet_mode='extended',\n",
    "                           count=1000,\n",
    "                           lang = \"en\").items():\n",
    "     # Write a row to the CSV file. I use encode UTF-8\n",
    "        csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "        print (tweet.created_at, tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all special characters\n",
    "list1=[]\n",
    "for i in k1:\n",
    "    print(i)\n",
    "    d = re.sub(r'[^a-zA-Z0-9]', ' ',i)\n",
    "    list1.append(d)\n",
    "list1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.to_csv(\"C:\\\\Users\\\\HP\\\\Downloads\\\\Twitter csv files\\\\1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date&Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time=df['Date&Time'][0:]\n",
    "date_time.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2=df['User_id&msg'][1:]\n",
    "print(str2.head(3))\n",
    "lst= []\n",
    "for i in str2:\n",
    "    User_id=re.findall(':[^.]+',i)\n",
    "    if User_id:\n",
    "        lst.append(User_id[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "newlst = []\n",
    "for j in lst:\n",
    "        a=j.replace('#',\" \")\n",
    "        a1=a.replace('\\\\n',\" \")\n",
    "        c=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', a1)\n",
    "        if c:\n",
    "            newlst.append(c)\n",
    "        #e=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', a1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take unique id\n",
    "anew=gp1.get_group('@tn_ajith')\n",
    "#print(anew)\n",
    "anew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a21=[]\n",
    "for i in anew['msg']:\n",
    "    #print(i)\n",
    "    a2=i.encode('utf8')\n",
    "    a21.append(a2)\n",
    "#a21    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=[]\n",
    "for x in a21:\n",
    "    k = x.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "    k1.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#installation of unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "def deEmojify(k1):\n",
    "    returnString = \"\"\n",
    "\n",
    "    for j in k1:\n",
    "        try:\n",
    "            j.encode(\"ascii\")\n",
    "            returnString += j\n",
    "            returnString\n",
    "        except UnicodeEncodeError:\n",
    "            replaced = unidecode(str(j))\n",
    "            if replaced != '':\n",
    "                returnString += replaced\n",
    "            else:\n",
    "                try:\n",
    "                     returnString += \"[\" + unicodedata.name(j) + \"]\"\n",
    "                except ValueError:\n",
    "                     returnString += \"[x]\"\n",
    "\n",
    "    return returnString\n",
    "emo=[]\n",
    "for i in k1:\n",
    "    emo1=deEmojify(i)\n",
    "    emo.append(emo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all special characters\n",
    "list1=[]\n",
    "for i in emo:\n",
    "    #print(i)\n",
    "    d = re.sub(r'[^a-zA-Z0-9]', ' ',i)\n",
    "    list1.append(d)\n",
    "#list1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(Type): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(Type) \n",
    "      \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
    "    print( sentiment_dict['neg']*100, \"% Negative\") \n",
    "    print(sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    print(sentiment_dict['pos']*100, \"% Positive\") \n",
    "  \n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "    #print('')\n",
    "    \n",
    "  \n",
    "    # decide sentiment as positive, negative and neutral depending on compound value\n",
    "    #Here we can fix any compound range\n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "  \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "  \n",
    "    else : \n",
    "        print(\"Neutral\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" : \n",
    "    \n",
    "    for i in list1:\n",
    "        print(sentiment_scores(i))\n",
    "        ptweets = [tweet for tweet in tweets  if sentiment_dict['compound'] >= 0.05 ] \n",
    "        print(ptweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(Type): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(Type) \n",
    "      \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
    "    print( sentiment_dict['neg']*100, \"% Negative\") \n",
    "    print(sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    print(sentiment_dict['pos']*100, \"% Positive\") \n",
    "  \n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "    #print('')\n",
    "    \n",
    "  \n",
    "    # decide sentiment as positive, negative and neutral depending on compound value\n",
    "    #Here we can fix any compound range\n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "  \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "  \n",
    "    else : \n",
    "        print(\"Neutral\") \n",
    "    #ptw=[if sentiment_dict['compound'] >= 0.05 ,'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" : \n",
    "    \n",
    "    for i in list1:\n",
    "        print(sentiment_scores(i))\n",
    "        #ptweets = [tweet for tweet in tweets  if sentiment_dict['compound'] >= 0.05 ] \n",
    "        #print(sentiment_scores(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/almost-real-time-twitter-sentiment-analysis-with-tweep-vader-f88ed5b93b1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Msg Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d= pd.DataFrame(newlst)\n",
    "d.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['date&time'] = 'date_time'         #astype there to change type\n",
    "d.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=d[0].to_dict( )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd=b'Within minutes Comments \\\\xf0\\\\x9f\\\\x98\\\\xb1\\\\xf0\\\\x9f\\\\x98\\\\xb1\\\\xf0\\\\x9f\\\\x98\\\\xb1\\\\xf0\\\\x9f\\\\x98\\\\xb1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = rnd.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=a.values()\n",
    "val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = val.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Str =  b'Need the \\xe2\\x80\\xa6' \n",
    "print(type(Str))\n",
    "Str.decode('utf8')\n",
    "\n",
    "\n",
    "#print (str(Str, 'utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "vall= val.apply(ast.literal_eval).str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertext = d.to_excel('Downloads.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d, columns=['date_time','user_id1'])\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps: unique word list,stopwords,stemmimg,lematization,tokenization,countvector,tf,idf,special characters,stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa='Within minutes Comments ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±  $#@...,,,;:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = aaa.str.replace('[^a-zA-Z#]',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctions,numbers,special characters\n",
    "d[0] = d[0].str.replace('[^a-zA-Z#]',\" \")\n",
    "d.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Short words\n",
    "d[0] = d[0].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokenized_tweet = d[0].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "d[0] = tokenized_tweet\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(Type): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(Type) \n",
    "      \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
    "    print( sentiment_dict['neg']*100, \"% Negative\") \n",
    "    print(sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    print(sentiment_dict['pos']*100, \"% Positive\") \n",
    "  \n",
    "    print(\"Sentence Overall Rated As\", end = \" \") \n",
    "  \n",
    "    # decide sentiment as positive, negative and neutral \n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "  \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "  \n",
    "    else : \n",
    "        print(\"Neutral\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" : \n",
    "    \n",
    "    print(d[0])\n",
    "sentiment_scores(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "b = b'\\\\xf0\\\\x9f\\\\x98\\\\x8e'\n",
    "b1 = b.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "print(c)\n",
    "b1 = b.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "D = {b'\\\\xf0\\\\x9f\\\\x98\\\\x8e' : 2, b'\\\\xF0\\\\x9F\\\\x98\\\\x82': 1, b'\\\\xF0\\\\x9F\\\\x98\\\\x86': 2, b'\\\\xF0\\\\x9F\\\\x98\\\\x89': 1, b'\\\\xF0\\\\x9F\\\\x8D\\\\xB5': 2, b'\\\\xF0\\\\x9F\\\\x8D\\\\xB0': 4, b'\\\\xF0\\\\x9F\\\\x8D\\\\xAB': 2, b'\\\\xF0\\\\x9F\\\\x8D\\\\xA9': 2, b'\\\\xF0\\\\x9F\\\\x98\\\\x98': 1, b'\\\\xE2\\\\x98\\\\xBA': 33, b'\\\\xE2\\\\x98\\\\x95': 1}\n",
    "for k,v in D.items():\n",
    "    k = k.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "    try:\n",
    "        n = ud.name(k)\n",
    "    except ValueError:\n",
    "        n = 'no such name'\n",
    "    print(k,ascii(k),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\xf0\\x9f\\x98\\x8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "D = {b'\\\\xf0\\\\x9f\\\\x98\\\\x8e' : 2}\n",
    "for k,v in D.items():\n",
    "    k = k.decode('unicode-escape').encode('latin1').decode('utf8')\n",
    "    try:\n",
    "        n = ud.name(k)\n",
    "    except ValueError:\n",
    "        n = 'no such name'\n",
    "    print(k,ascii(k),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trail2 =  'm december Thala Ajith sir recent picture with family !  AadvikAjith is so cute asusual like Dad !  That Costume wear for Aadvik Ajit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trail1 = 'm december Thala Ajith sir recent picture with family !  AadvikAjith is so cute asusual like Dad !  That Costume wear for Aadvik Ajit\\\\xe2\\\\x80\\\\xa6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyemoji = b'\\\\xf0\\\\x9f\\\\x98\\\\x8e'\n",
    "input1 = 'today is bad day'\n",
    "withemoji='ThalaAjith ðŸ˜ŽDance Style look SuperðŸ˜ðŸ’˜Kutty Thala AadvikAjith Standing look ðŸ˜˜ðŸ’˜'\n",
    "withoutemoji='ThalaAjith Dance Style look Super Kutty Thala AadvikAjith Standing look '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
